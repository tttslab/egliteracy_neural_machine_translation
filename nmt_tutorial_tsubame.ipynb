{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stuffed-encounter",
   "metadata": {},
   "source": [
    "### 実験条件の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# データ量を全体の1%, 20%, 60% 100%から選択する\n",
    "UTIL_RATE=20\n",
    "allowed_util_rate = [1, 20, 60, 100]\n",
    "\n",
    "if(UTIL_RATE not in allowed_util_rate):\n",
    "    print('データ量の設定が誤っています')\n",
    "    sys.exit()\n",
    "\n",
    "%env UTIL_RATE = $UTIL_RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-visibility",
   "metadata": {},
   "source": [
    "### 割り当てられたGPUの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-privilege",
   "metadata": {},
   "source": [
    "### 必要なライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m pip install --user ConfigArgParse\n",
    "python3 -m pip install --user torchtext==0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-property",
   "metadata": {},
   "source": [
    "### パスなど環境変数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pwd = os.getcwd()\n",
    "### This directory contains libraries and exexution files.\n",
    "APPS_DIR = pwd + '/apps'\n",
    "%env APPS_DIR=$APPS_DIR\n",
    "### This directory contains training, development and test set\n",
    "DATA_DIR = pwd + '/dataset'\n",
    "%env DATA_DIR=$DATA_DIR\n",
    "### This directory contains output directions.\n",
    "WORK_DIR = pwd + '/work'\n",
    "%env WORK_DIR=$WORK_DIR\n",
    "\n",
    "#japanese -> english\n",
    "%env LANG_PAIR=ja-en\n",
    "#english -> japanese\n",
    "#%env LANG_PAIR=en-ja\n",
    "\n",
    "%env DATASET=$DATA_DIR/kftt-data-1.0/data/tok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-constitution",
   "metadata": {},
   "source": [
    "### 使用するライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-butter",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load gcc\n",
    "\n",
    "echo \"download and make apps...\"\n",
    "\n",
    "mkdir -p ${APPS_DIR}\n",
    "cd ${APPS_DIR}\n",
    "\n",
    "git clone https://github.com/moses-smt/mosesdecoder.git\n",
    "git clone https://github.com/OpenNMT/OpenNMT-py.git -b 1.2.0\n",
    "git clone https://github.com/neubig/kytea.git\n",
    "cd kytea\n",
    "autoreconf -i\n",
    "./configure --prefix=${APPS_DIR}/kytea\n",
    "make\n",
    "make install\n",
    "\n",
    "echo \"finish apps preparation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-melissa",
   "metadata": {},
   "source": [
    "### 使用するデータのダウンロードとデータ量調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p $WORK_DIR/input\n",
    "mkdir -p ${DATA_DIR}\n",
    "\n",
    "# Download data set\n",
    "wget -O ${DATA_DIR}/kftt-data-1.0.tar.gz http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n",
    "cd ${DATA_DIR}\n",
    "# Uncompress\n",
    "tar xvzf kftt-data-1.0.tar.gz\n",
    "\n",
    "paste \\\n",
    "  ${DATASET}/kyoto-train.cln.en \\\n",
    "  ${DATASET}/kyoto-train.cln.ja |\n",
    "  shuf > ${DATASET}/train.shuf\n",
    "\n",
    "if [ $UTIL_RATE == 1 ]; then\n",
    "    echo \"data size : 1%\"\n",
    "    head -n $((`cat ${DATASET}/train.shuf | wc -l`/100)) ${DATASET}/train.shuf | cut -f 1 > ${DATASET}/train.en\n",
    "    head -n $((`cat ${DATASET}/train.shuf | wc -l`/100)) ${DATASET}/train.shuf | cut -f 2 > ${DATASET}/train.ja\n",
    "elif [ $UTIL_RATE == 20 ]; then\n",
    "    echo \"data size : 20%\"\n",
    "    head -n $((`cat ${DATASET}/train.shuf | wc -l`/5)) ${DATASET}/train.shuf | cut -f 1 > ${DATASET}/train.en\n",
    "    head -n $((`cat ${DATASET}/train.shuf | wc -l`/5)) ${DATASET}/train.shuf | cut -f 2 > ${DATASET}/train.ja\n",
    "elif [ $UTIL_RATE == 60 ]; then\n",
    "    echo \"data size : 60%\"\n",
    "    head -n $((3*`cat ${DATASET}/train.shuf | wc -l`/5)) ${DATASET}/train.shuf | cut -f 1 > ${DATASET}/train.en\n",
    "    head -n $((3*`cat ${DATASET}/train.shuf | wc -l`/5)) ${DATASET}/train.shuf | cut -f 2 > ${DATASET}/train.ja\n",
    "else\n",
    "    echo \"data size : 100%\"\n",
    "    cut -f 1 ${DATASET}/train.shuf > ${DATASET}/train.en\n",
    "    cut -f 2 ${DATASET}/train.shuf > ${DATASET}/train.ja\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-stability",
   "metadata": {},
   "source": [
    "### 使用するデータのサンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "train_name=train\n",
    "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
    "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
    "\n",
    "sed -n 1P ${DATASET}/${train_name}.${src}\n",
    "sed -n 1P ${DATASET}/${train_name}.${trg}\n",
    "sed -n 2P ${DATASET}/${train_name}.${src}\n",
    "sed -n 2P ${DATASET}/${train_name}.${trg}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-maple",
   "metadata": {},
   "source": [
    "### OpenNMT-pyライブラリで前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ${APPS_DIR}/OpenNMT-py\n",
    "suffix=\"en-ja ja-en\"\n",
    "train_name=train\n",
    "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
    "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
    "    python preprocess.py \\\n",
    "    -train_src ${DATASET}/${train_name}.${src} \\\n",
    "    -train_tgt ${DATASET}/${train_name}.${trg} \\\n",
    "    -valid_src ${DATASET}/kyoto-dev.${src} \\\n",
    "    -valid_tgt ${DATASET}/kyoto-dev.${trg} \\\n",
    "    -save_data ${DATA_DIR}/dicts-${train_name}-${LANG_PAIR} \\\n",
    "    -src_words_min_frequency 5 \\\n",
    "    -tgt_words_min_frequency 5 \\\n",
    "    -src_seq_length 40 \\\n",
    "    -tgt_seq_length 40\n",
    "#train_src, train_tgt, valid_src, valid_tgt: text\n",
    "#save_data: bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-repository",
   "metadata": {},
   "source": [
    "### 前処理後の学習データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {APPS_DIR}/OpenNMT-py\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "data_path = os.path.join(DATA_DIR, 'dicts-train-ja-en.train.0.pt')\n",
    "vocab_path = os.path.join(DATA_DIR, 'dicts-train-ja-en.vocab.pt')\n",
    "#学習データ。\n",
    "#data = torch.load('/content/dataset/dicts-train-ja-en.train.0.pt')\n",
    "data = torch.load(data_path)\n",
    "\n",
    "#vocab: データの種類とその処理方法の定義。\n",
    "#vocab = torch.load('/content/dataset/dicts-train-ja-en.vocab.pt')\n",
    "vocab = torch.load(vocab_path)\n",
    "\n",
    "print(\"学習データ数\", len(data))\n",
    "print(\"\")\n",
    "print(\"data[0]\")\n",
    "print(\"index\", data[0].indices)\n",
    "print(\"翻訳前\", data[0].src)\n",
    "print(\"翻訳後\", data[0].tgt)\n",
    "print(\"\")\n",
    "print(\"data[1]\")\n",
    "print(\"index\", data[1].indices)\n",
    "print(\"翻訳前\", data[1].src)\n",
    "print(\"翻訳後\", data[1].tgt)\n",
    "print(\"\")\n",
    "print(\"これが数値に変換され、以下の形式でニューラルネットに渡される。\")\n",
    "print(vocab['src'].fields[0][1].process((data[0].src[0],data[1].src[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-adams",
   "metadata": {},
   "source": [
    "### ニューラルネットの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "start_time=`date +%s`\n",
    "\n",
    "TIME_PATH=$(pwd)/train.time.log\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_NAME=train\n",
    "MODEL_DIR=${WORK_DIR}/models\n",
    "\n",
    "NUM_EPOCH=10\n",
    "\n",
    "mkdir -p ${MODEL_DIR}\n",
    "\n",
    "NUM_DATA=$(cat ${DATASET}/${TRAIN_NAME}.en | wc -l)\n",
    "\n",
    "cd ${APPS_DIR}/OpenNMT-py\n",
    "\n",
    "batch_size=256\n",
    "num_steps=$((${NUM_DATA}*${NUM_EPOCH}/${batch_size}))\n",
    "#モデルは左のファイルボタンから、\n",
    "#encoder: apps/OpenNMT-py/onmt/encoders/rnn_encoder.py (RNNEncoder)\n",
    "#decoder: apps/OpenNMT-py/onmt/encoders/decoder.py (StdRNNDecoder)\n",
    "#を参照。または授業資料を参照。\n",
    "python train.py \\\n",
    "    -data ${DATA_DIR}/dicts-${TRAIN_NAME}-${LANG_PAIR} \\\n",
    "    -save_model ${MODEL_DIR}/${LANG_PAIR} \\\n",
    "    -layers 2 \\\n",
    "    -rnn_size 500 \\\n",
    "    -word_vec_size 300 \\\n",
    "    -optim adam \\\n",
    "    -learning_rate 0.001 \\\n",
    "    -dropout 0.3 \\\n",
    "    -batch_size ${batch_size} \\\n",
    "    -report_every 1 \\\n",
    "    -save_checkpoint_steps ${num_steps} \\\n",
    "    -train_steps ${num_steps} \\\n",
    "    -gpu_rank 0 |& awk '(NR%30==0 || NR < 70){print}' #出力を少し抑制（重くなるので）\n",
    "cp ${MODEL_DIR}/${LANG_PAIR}_step_${num_steps}.pt ${MODEL_DIR}/${LANG_PAIR}_final.pt\n",
    "\n",
    "\n",
    "end_time=`date +%s`\n",
    "time=$((end_time - start_time))\n",
    "echo \"${time} (sec)\" >& $TIME_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-asthma",
   "metadata": {},
   "source": [
    "### 結果の評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "\n",
    "MODELS=${WORK_DIR}/models\n",
    "OUT_DIR=${WORK_DIR}/outputs\n",
    "USRDIR=${WORK_DIR}/input\n",
    "SCORE_PATH=$(pwd)/score.txt\n",
    "\n",
    "mkdir -p ${OUT_DIR}\n",
    "\n",
    "cd ${APPS_DIR}/OpenNMT-py\n",
    "\n",
    "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
    "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
    "\n",
    "#学習したモデルを使ってsize1のビームサーチにより翻訳結果を取得。\n",
    "python translate.py \\\n",
    "    -model ${MODELS}/${LANG_PAIR}_final.pt \\\n",
    "    -src ${DATASET}/kyoto-test.${src} \\\n",
    "    -output ${OUT_DIR}/test.${trg} \\\n",
    "    -gpu 0 \\\n",
    "    -beam_size 1 \\\n",
    "    -batch_size 512 \\\n",
    "    -verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-north",
   "metadata": {},
   "source": [
    "### 好きな文章を翻訳してみる\n",
    "#### （以下の`echo 何らかの文章をここに書く。 > ${USRDIR}/user.${src}`を書き換えて上のセル実行ボタン▶を押してみよう）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "\n",
    "MODELS=${WORK_DIR}/models\n",
    "OUT_DIR=${WORK_DIR}/outputs\n",
    "USRDIR=${WORK_DIR}/input\n",
    "SCORE_PATH=$(pwd)/score.txt\n",
    "\n",
    "cd ${APPS_DIR}/OpenNMT-py\n",
    "\n",
    "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
    "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
    "\n",
    "for i in 1 2 3 4 5; do\n",
    "  echo [評価データの翻訳結果 $i]\n",
    "  sed -n ${i}P ${OUT_DIR}/test.${trg}\n",
    "  echo [評価データの正しい翻訳結果 $i]\n",
    "  sed -n ${i}P ${DATASET}/kyoto-test.${trg}\n",
    "done\n",
    "\n",
    "#bleuスコア算出。\n",
    "perl ${APPS_DIR}/mosesdecoder/scripts/generic/multi-bleu.perl \\\n",
    "    ${DATASET}/kyoto-test.${trg} \\\n",
    "    < ${OUT_DIR}/test.${trg} \\\n",
    "    1> ${OUT_DIR}/result_${LANG_PAIR}.bleu \\\n",
    "    2> /dev/null\n",
    "\n",
    "cat ${OUT_DIR}/result_${LANG_PAIR}.bleu | sed -r 's/(BLEU = [0-9]*\\.[0-9]*), .*/\\1/g' | tee ${SCORE_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODELS=${WORK_DIR}/models\n",
    "OUT_DIR=${WORK_DIR}/outputs\n",
    "USRDIR=${WORK_DIR}/input\n",
    "SCORE_PATH=$(pwd)/score.txt\n",
    "src=`echo ${LANG_PAIR} | awk -F\"-\" '{print $1}'`\n",
    "trg=`echo ${LANG_PAIR} | awk -F\"-\" '{print $2}'`\n",
    "\n",
    "echo 何らかの文章をここに書く。 > ${USRDIR}/user.${src}\n",
    "\n",
    "if [ ! -e ${USRDIR}/user.${src} ]\n",
    "then\n",
    "    echo \"${USRDIR}/user.${src} does not exist.\"\n",
    "    exit\n",
    "fi\n",
    "\n",
    "\n",
    "#tokenに分割。\n",
    "if [ ${src} = \"ja\" -a ${src} != \"en\" ]\n",
    "then\n",
    "    # Tokenize Japanese sentences\n",
    "    bash ${APPS_DIR}/kytea/src/bin/kytea \\\n",
    "    < ${USRDIR}/user.${src} |\\\n",
    "    sed 's/\\/[^ ]\\+//g' \\\n",
    "    > ${USRDIR}/user.tok.${src}\n",
    "elif [ ${src} = \"en\" -a ${src} != \"ja\" ]\n",
    "then\n",
    "    # Tokenize English sentences\n",
    "    perl ${APPS_DIR}/mosesdecoder/scripts/tokenizer/tokenizer.perl \\\n",
    "    -l en \\\n",
    "    < ${USRDIR}/user.${src} \\\n",
    "    > ${USRDIR}/user.tok.${src}\n",
    "else\n",
    "echo \"Language: ${src} is undefined.\"\n",
    "    continue\n",
    "fi\n",
    "\n",
    "#分割結果。\n",
    "cat ${USRDIR}/user.tok.${src}\n",
    "\n",
    "#あとは同様に翻訳する。\n",
    "python translate.py \\\n",
    "    -model ${MODELS}/${LANG_PAIR}_final.pt \\\n",
    "    -src ${USRDIR}/user.tok.${src} \\\n",
    "    -output ${OUT_DIR}/user.${trg} \\\n",
    "    -gpu 0 \\\n",
    "    -beam_size 1 \\\n",
    "    -batch_size 512 \\\n",
    "    -verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat train.time.log\n",
    "!cat score.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
